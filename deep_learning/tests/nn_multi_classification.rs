/* -*- tab-width:4 -*- */

use std::fmt::Display;
use std::rc::Rc;
use std::error::Error;
use core::array::from_fn;
use rand::prelude::SliceRandom;

use num::ToPrimitive;

use linear_transform::Tensor;
use deep_learning::neural_network::NeuralNetwork;
use deep_learning::datasets::*;
use deep_learning::neural_network::model::MLPActivator;
use deep_learning::neural_network::optimizer::{SGD,Optimizer,NNOptimizer};

use plotters::prelude::{BitMapBackend,ChartBuilder};
use plotters::prelude::{Circle,Cross,Rectangle,TriangleMarker,EmptyElement,PathElement,
						SeriesLabelPosition};
use plotters::prelude::full_palette::*;
use plotters::drawing::IntoDrawingArea;
use plotters::style::{IntoFont,Color};

#[derive(Debug)]
enum MyError {
	StringMsg(String)
}

impl Display for MyError {
	fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
		use self::MyError::*;
		match self {
			StringMsg(s) => write!(f, "{}", s)
		}
	}
}

impl Error for MyError {}

fn numerical_grad<T>(f:&(dyn Fn(&Tensor::<T>) -> Tensor::<T>),
					 x:&Tensor::<T>,
					 delta:T) -> Tensor::<T>
where T:Clone+Copy+Display+num::Num {

	let mut pos:usize = 0;
	let mut v:Vec<T> = Vec::new();

	for x0 in x.buffer().iter() {
		let mut forward_x0   = x.clone();
		let mut backward_x0  = x.clone();
		forward_x0.replace_element_by_index(pos, *x0-delta);
		backward_x0.replace_element_by_index(pos, *x0+delta);
		let forward_fx0  = f(&forward_x0);
		let backward_fx0 = f(&backward_x0);
		let diff = (backward_fx0 - forward_fx0).sum(&[1,1])[vec![0,0]];
		v.push(diff/((num::one::<T>()+num::one::<T>())*delta));
		pos += 1;
	}

	Tensor::<T>::from_vector(x.shape().to_vec(), v)
}

#[test]
fn spiral_multi_classification_test() -> Result<(),Box<dyn std::error::Error>> {
    let max_epoch = 300;
    let batch_size = 30;
    let num_of_class = 3;
    let num_of_data = 100;
    let num_of_alldata = num_of_class * num_of_data;

	let (xs,ts) = spiral::get_2d_dataset::<f64>(num_of_class,100);
	//println!("{}",xs);
	//println!("{}",ts);
	let render_backend = BitMapBackend::new("spiral_graph.png", (640, 480)).into_drawing_area();
	render_backend.fill(&WHITE)?;

	let mut chart_builder = ChartBuilder::on(&render_backend)
		.caption("spiral", ("sans-serif", 40).into_font())
		.margin(5)
		.x_label_area_size(30)
		.y_label_area_size(30)
		.build_cartesian_2d(-1.56f32..1.56f32, -1.2f32..1.2f32)?;
	chart_builder.configure_mesh().disable_x_mesh().disable_y_mesh().draw()?;

	let class_iter =
			xs.iter().zip(ts.iter()).filter(|(xst,t)| t[vec![0,0]] == 0.0 as f64);
	chart_builder.draw_series(class_iter.map(|(xst,t)| {
		let x = xst[vec![0,0]] as f32;
		let y = xst[vec![0,1]] as f32;
		EmptyElement::at((x,y)) + Circle::new((0,0), 2, GREEN.filled())
	}))?;
	let class_iter =
			xs.iter().zip(ts.iter()).filter(|(xst,t)| t[vec![0,0]] == 1.0 as f64);
	chart_builder.draw_series(class_iter.map(|(xst,t)| {
		let x = xst[vec![0,0]] as f32;
		let y = xst[vec![0,1]] as f32;
		EmptyElement::at((x,y)) + TriangleMarker::new((0,0), 2, RED.filled())
	}))?;
	let class_iter =
		xs.iter().zip(ts.iter()).filter(|(xst,t)| t[vec![0,0]] == 2.0 as f64);
	chart_builder.draw_series(class_iter.map(|(xst,t)| {
		let x = xst[vec![0,0]] as f32;
		let y = xst[vec![0,1]] as f32;
		EmptyElement::at((x,y)) + Cross::new((0,0), 2, CYAN.filled())
	}))?;
	/*
	let class_iter =
		xs.iter().zip(ts.iter()).filter(|(xst,t)| t[vec![0,0]] == 3.0 as f64);
	chart_builder.draw_series(class_iter.map(|(xst,t)| {
		let x = xst[vec![0,0]] as f32;
		let y = xst[vec![0,1]] as f32;
		EmptyElement::at((x,y)) + Rectangle::new([(0,0),(2,2)], BLACK.filled())
	}))?;
*/
	render_backend.present()?;

	{
		let mut nn = NeuralNetwork::<f64>::new();
		let x = nn.create_constant("input_x", Tensor::<f64>::zero(&[batch_size,2]));
		let t = nn.create_constant("teacher", Tensor::<f64>::zero(&[batch_size,1]));

		let mut mlp_model = nn.create_mlp_model("MLP1", &[10,num_of_class], MLPActivator::Sigmoid);
		let classfied_result = nn.model_set_inputs(&mut mlp_model, vec![Rc::clone(&x)]);
		let mut optimizer = NNOptimizer::<f64>::new(Optimizer::SGD(SGD::new(1.0)),
													&mlp_model);
		let loss = nn.softmax_cross_entropy_error(Rc::clone(&classfied_result[0]), Rc::clone(&t));
		let count_of_training =  num_of_alldata/batch_size;

		nn.backward_propagating(0)?;

		for epoch in 0..max_epoch {
			let mut sum_loss:f64 = 0.0;
			let perm_table = {
				let mut v = (0..(num_of_data*num_of_class)).collect::<Vec<usize>>();
				v.shuffle(&mut nn.get_rng());
				v
			};
			for i in 0..count_of_training {
				// make mini batch
				let batch_index = &perm_table[i*batch_size..(i+1)*batch_size];
				let batch_x = xs.selector(batch_index);
				let batch_t = ts.selector(batch_index);

				x.borrow_mut().assign(batch_x.clone());
				t.borrow_mut().assign(batch_t.clone());

				//println!("{}",x.borrow().ref_signal());
				//println!("{}",t.borrow().ref_signal());
				nn.forward_propagating(0)?;
				nn.forward_propagating(1)?;

				println!("loss {} {}", loss.borrow().ref_signal()[vec![0,0]], batch_size);
				sum_loss += loss.borrow().ref_signal()[vec![0,0]] * (batch_size as f64);

				optimizer.update()?;
			}
			println!("sum_loss {} {}",sum_loss,num_of_alldata);
			println!("epoch {}, loss {}", epoch, sum_loss / (num_of_alldata as f64));
		}
	}

	Ok(())
}

fn fixed_sprial_data_set() -> (Tensor<f64>, Tensor<f64>) {
	let data:Vec<f64> = {
		let data = [
			[-0.22394885,  0.05241101],
			[ 0.10637081, -0.10576035],
			[ 0.34734264,  0.5494116 ],
			[-0.30824462, -0.8028607 ],
			[-0.15145358,  0.3375823 ],
			[-0.74260926,  0.47215626],
			[ 0.6181036,   0.442208  ],
			[ 0.13746658, -0.08187149],
			[ 0.5703474,   0.5609847 ],
			[ 0.2517982,   0.14386679],
			[-0.24811222,  0.26084542],
			[ 0.15063146,  0.1868426 ],
			[ 0.5008086,  -0.17346679],
			[ 0.42675576, -0.05272124],
			[ 0.00491708, -0.33996445],
			[-0.00097699,  0.00995216],
			[ 0.10108227, -0.13668348],
			[-0.13840458,  0.37529212],
			[-0.00837021, -0.00547171],
			[ 0.15359965,  0.12809038],
			[-0.2772819 ,  0.03891963],
			[ 0.8451174 ,  0.2065832 ],
			[ 0.27190292,  0.18699946],
			[-0.7831353 , -0.5552469 ],
			[-0.16242598, -0.40892273],
			[-0.7781106 ,  0.05425706],
			[-0.24501143,  0.04969303],
			[ 0.73510563,  0.4064723 ],
			[ 0.1811798 , -0.70715904],
			[ 0.13923591,  0.12928016],
			[-2.33354911e-01,  1.14653774e-01],
			[-8.09110165e-01,  1.33194506e-01],
			[ 9.88995433e-01, -4.45871502e-02],
			[-1.39120460e-01,  4.80255663e-01],
			[ 3.70493919e-01,  5.82094729e-01],
			[ 9.18575346e-01,  5.11797182e-02],
			[ 9.08727646e-01,  2.76973128e-01],
			[ 6.46809101e-01,  5.04021823e-01],
			[-1.13882385e-01, -8.62514257e-01],
			[-1.00885399e-01, -3.55980515e-01],
			[-2.68359423e-01,  3.85853887e-01],
			[ 4.62137282e-01, -3.66782159e-01],
			[-1.58687178e-02,  5.39766788e-01],
			[-6.62681818e-01, -1.92231089e-01],
			[-7.84879804e-01,  1.54801995e-01],
			[ 1.25355586e-01,  5.86758852e-01],
			[ 5.12668258e-03,  1.93317644e-02],
			[ 1.89873278e-01,  1.11122184e-01],
			[ 4.37091216e-02, -7.38708019e-01],
			[ 3.26863602e-02, -2.07440600e-01],
			[-2.08488569e-01,  5.30502141e-01],
			[ 3.01335216e-01,  5.41845977e-01],
			[ 1.94660928e-02, -3.59473318e-01],
			[ 1.50106937e-01, -1.74263909e-01],
			[-2.70172387e-01,  1.05389200e-01],
			[ 9.41914099e-04,  4.99911271e-02],
			[-6.95635676e-01, -2.21339032e-01],
			[-1.95859388e-01, -8.88672650e-01],
			[-1.25241682e-01,  4.11356926e-01],
			[-7.10909605e-01,  3.44539642e-01],
			[-0.41255736, -0.36372572],
			[ 0.03845111, -0.01102326],
			[-0.13981389, -0.00721657],
			[-0.02386156, -0.01818313],
			[ 0.3031688,   0.06472   ],
			[ 0.08284235,  0.10018555],
			[-0.65090775,  0.62154573],
			[ 0.09607905, -0.08757178],
			[ 0.5055555,   0.59398115],
			[ 0.4110389,  -0.4092029 ],
			[-0.0271639,  -0.02936192],
			[-0.18852998, -0.37530845],
			[-0.10498876,  0.44785863],
			[-0.0,         0.0       ],
			[ 0.06264956,  0.6067743 ],
			[ 0.31354076,  0.58076864],
			[ 0.36761343, -0.48678574],
			[-0.58700913, -0.79719526],
			[-0.26706883,  0.27032247],
			[ 0.34195596,  0.07460645],
			[-0.08318522,  0.5031702 ],
			[ 0.13791645, -0.76771027],
			[-0.18813577, -0.79812586],
			[-0.74082303, -0.59471107],
			[ 0.04958905, -0.00639738],
			[ 0.34992874,  0.6062589 ],
			[-0.19780748, -0.85748017],
			[ 0.08061991,  0.1609361 ],
			[-0.21432792,  0.04963413],
			[-0.7474886,   0.44515252],
			[-0.18253154,  0.05274695],
			[-0.07488721,  0.51457936],
			[-0.07018702, -0.7065223 ],
			[-0.30727252,  0.14554588],
			[-0.84679544,  0.35963517],
			[-0.7121174 ,  0.42636698],
			[-0.61125225, -0.29794413],
			[-0.28640968, -0.78966415],
			[ 0.43358243, -0.18140091],
			[ 0.39779076, -0.09931022],
			[ 0.4752903 , -0.06707557],
			[-0.12995958, -0.00324155],
			[-0.15293403, -0.4444223 ],
			[-0.01839967, -0.00783914],
			[-0.2656141 , -0.39981142],
			[ 0.9336293 , -0.29788658],
			[-0.76282126,  0.10489876],
			[-0.23974584,  0.25499398],
			[ 0.1625132 , -0.68087405],
			[-0.15886047, -0.01906173],
			[-0.26522237, -0.45886502],
			[ 0.44608194,  0.6029187 ],
			[ 0.30444184, -0.5855896 ],
			[ 0.13898164,  0.15742968],
			[-0.02893586,  0.58929   ],
			[-0.39722073, -0.8519482 ],
			[-0.01749143, -0.3295361 ],
			[ 0.05059152, -0.30584392],
			[ 0.1297094 , -0.15223493],
			[-0.0401785 , -0.7989904 ],
			[-0.14900257,  0.529432  ],
			[ 0.42734203, -0.10477977],
			[ 0.09690688, -0.24126554],
			[-0.09433582, -0.38871682],
			[ 0.02253613,  0.05560686],
			[-0.07588156, -0.02533749],
			[-0.6793856 , -0.7062827 ],
			[-0.14406797,  0.65432745],
			[ 0.13273749, -0.19995189],
			[-0.6994348 ,  0.0281238 ],
			[ 0.00246038, -0.37999204],
			[-0.23457344, -0.4415601 ],
			[-0.65559965, -0.6454371 ],
			[-0.03843534, -0.6688967 ],
			[ 0.05652277,  0.5772393 ],
			[-0.81171393,  0.4113642 ],
			[-0.00570804,  0.5299693 ],
			[ 0.08143438, -0.05803829],
			[ 0.9365552 ,  0.08040111],
			[ 0.6741878 ,  0.517659  ],
			[ 0.13424662, -0.21089771],
			[ 0.43614882, -0.22332534],
			[ 0.36799538, -0.5836775 ],
			[-0.25087747,  0.09980225],
			[-0.17861071,  0.11044552],
			[ 0.09220015, -0.05999277],
			[-0.00640535, -0.759973  ],
			[ 0.21005045,  0.15322797],
			[-0.14541428, -0.8374692 ],
			[-0.10568184, -0.03051801],
			[-0.11285946, -0.04077674],
			[ 0.27801412, -0.58754414],
			[-0.30477002, -0.4816796 ],
			[-0.19354802,  0.4392484 ],
			[-0.00198436,  0.5599965 ],
			[ 0.2805128 , -0.4846778 ],
			[-0.6302627 ,  0.72413325],
			[ 0.6405069 ,  0.60344917],
			[-0.2891464 ,  0.26171428],
			[ 0.7107062 ,  0.42871517],
			[ 0.43733358, -0.10601573],
			[ 0.02032876,  0.08767407],
			[-0.9223507 ,  0.18129861],
			[ 0.662295  ,  0.3070266 ],
			[ 0.42443117, -0.33385354],
			[-0.21264821, -0.39658636],
			[ 0.05997716, -0.00165524],
			[ 0.43561164, -0.45512912],
			[ 0.13235243,  0.6261652 ],
			[ 0.10349102, -0.74282545],
			[-0.04843636, -0.34663224],
			[ 0.3396491 ,  0.01544307],
			[ 0.49350116,  0.51044744],
			[ 0.05984409,  0.0801167 ],
			[-0.16978921,  0.00846301],
			[-0.74176145,  0.11086001],
			[ 0.12092289, -0.70977294],
			[-0.41539234, -0.43295404],
			[-0.73185277,  0.20491832],
			[ 0.02616625, -0.01467403],
			[ 0.9229654,   0.11417017],
			[ 0.2721572,   0.7416404 ],
			[ 0.09332082,  0.11743604],
			[ 0.3952812 , -0.45138982],
			[ 0.29977617,  0.11195645],
			[ 0.4326635 , -0.5245973 ],
			[-0.6833807 , -0.6883972 ],
			[ 0.044994  , -0.31682098],
			[-0.32124844,  0.07549458],
			[-0.1952649 ,  0.39429888],
			[ 0.00979881, -0.00199584],
			[-0.05139323, -0.03096346],
			[ 0.15577695, -0.25638554],
			[-0.08034188, -0.45292956],
			[ 0.4547784 , -0.06911298],
			[-0.52873564, -0.7283122 ],
			[ 0.89618134,  0.08281892],
			[-0.0468118 , -0.3871804 ],
			[-0.01549666, -0.78984797],
			[-0.5878122 , -0.22666465],
			[-0.05060399, -0.427012  ],
			[ 0.9508962 , -0.13189542],
			[-0.17036279, -0.75091714],
			[ 0.08069741, -0.03984881],
			[-0.06727856, -0.0193286 ],
			[ 0.06190507,  0.09092724],
			[ 0.10985696,  0.08678392],
			[ 0.86612016,  0.20478243],
			[-0.5757059 ,  0.78068095],
			[ 0.42404062, -0.3502707 ],
			[ 6.0529912e-01,  5.3824991e-01],
			[ 5.0264323e-01,  4.5798451e-01],
			[ 1.9994073e-02,  4.8687626e-04],
			[-5.8656383e-01, -2.8007659e-01],
			[-4.5640576e-01,  8.3318293e-01],
			[ 6.2169746e-02, -3.2170214e-02],
			[-8.9592054e-02, -4.4421431e-02],
			[-8.4966242e-01, -2.3953741e-02],
			[-5.6674325e-01, -1.6401847e-01],
			[ 0.0000000e+00, -0.0000000e+00],
			[ 1.7171463e-01,  1.8169777e-01],
			[ 6.5284888e-03,  6.9694899e-02],
			[ 9.1778427e-01,  3.1396189e-01],
			[ 3.2280978e-02,  1.1557655e-01],
			[-4.0200278e-01, -3.8986379e-01],
			[-2.7397221e-01,  1.4504905e-01],
			[-4.5076111e-01, -3.6498550e-01],
			[ 1.4116073e-01,  1.8158647e-01],
			[ 3.6321253e-01,  6.2167245e-01],
			[-5.9743100e-01,  7.8941506e-01],
			[-1.7989218e-01,  6.2292074e-03],
			[-7.8472960e-01,  2.9966557e-01],
			[-3.8604334e-04,  2.9997516e-02],
			[ 2.2119455e-02, -2.6909241e-01],
			[ 8.1467050e-01,  2.7552122e-01],
			[-2.7497035e-01, -4.0557528e-01],
			[ 3.5615346e-01,  5.2485455e-02],
			[-7.1184760e-01,  4.8256913e-01],
			[-2.4758580e-01,  3.2680464e-01],
			[-3.3211905e-01, -4.2578977e-01],
			[-0.43632174, -0.8212937 ],
			[ 0.83352554,  0.3651509 ],
			[-0.5675895 , -0.3560086 ],
			[-0.57954365, -0.27153113],
			[ 0.07829762,  0.13953309],
			[ 0.36989382, -0.00886326],
			[ 0.11015151, -0.14236097],
			[-0.6003261 , -0.27424175],
			[ 0.2889293 , -0.40806845],
			[ 0.12399456,  0.11629853],
			[-0.28567183,  0.14419296],
			[ 0.07434364, -0.02954696],
			[-0.21861286,  0.35862017],
			[ 0.31820756,  0.6680898 ],
			[ 0.38677302, -0.34757248],
			[-0.66352814, -0.25264674],
			[ 0.27577567,  0.56643426],
			[ 0.46965688, -0.19880246],
			[ 0.41908243, -0.02774712],
			[ 0.02506495,  0.07597202],
			[ 0.38932335, -0.02296369],
			[ 0.01425096,  0.03737526],
			[-0.04677129, -0.01767616],
			[-0.4918118 , -0.3608617 ],
			[-0.0       , -0.0       ],
			[ 0.091654  , -0.1664318 ],
			[-0.77509767,  0.23520966],
			[-0.19678791,  0.03570037],
			[ 0.3977565 , -0.04230585],
			[-0.5489099 , -0.28826708],
			[-0.7281537 , -0.51175404],
			[-0.03366692, -0.40861538],
			[-0.23986334,  0.00809829],
			[-0.3786127 , -0.34169054],
			[-0.08773611,  0.4820813 ],
			[ 0.23121913,  0.1579168 ],
			[ 0.3282343 , -0.54941994],
			[ 0.10326176, -0.09453575],
			[ 0.25992823,  0.1497909 ],
			[ 0.5306231 , -0.20818052],
			[ 0.09373958, -0.07491923],
			[-0.84386814,  0.282819  ],
			[-0.29238784,  0.06715166],
			[-0.14677872,  0.42538926],
			[ 0.1374263 , -0.17179643],
			[ 0.23201972,  0.13808277],
			[ 0.37874952,  0.03080279],
			[ 0.03873516, -0.28740144],
			[-0.83592904,  0.4075814 ],
			[-0.29891637, -0.75282735],
			[-0.14588465, -0.0348951 ],
			[ 0.49455667, -0.3739167 ],
			[-0.08871835, -0.0151345 ],
			[-0.7074377 , -0.13391   ],
			[-0.33446026, -0.3981662 ],
			[-0.73952246, -0.02658003],
			[-0.20607458,  0.95808834],
			[-0.00116657, -0.8299992 ],
			[ 0.10375305, -0.26006788],
			[ 0.6075539 ,  0.47305202]
		];
		data.iter().fold(vec!(), |v, d| { let mut mut_v = v.clone();
										  mut_v.push(d[0]);
										  mut_v.push(d[1]);
										  mut_v })
	};
	let teacher:Vec<f64> = vec![
		1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0,
		1.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 1.0, 1.0, 0.0,
		1.0, 1.0, 0.0, 0.0, 2.0, 2.0, 1.0, 1.0, 0.0, 0.0,
		1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 2.0,
		1.0, 0.0, 1.0, 2.0, 2.0, 1.0, 0.0, 0.0, 0.0, 2.0,
		1.0, 1.0, 2.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0,
		2.0, 2.0, 1.0, 1.0, 0.0, 0.0, 2.0, 2.0, 1.0, 0.0,
		1.0, 2.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0,
		1.0, 0.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 1.0, 2.0,
		1.0, 1.0, 0.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0,
		0.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 1.0,
		2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0, 2.0, 2.0, 0.0,
		1.0, 0.0, 2.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 2.0,
		2.0, 2.0, 0.0, 0.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0,
		2.0, 0.0, 0.0, 1.0, 1.0, 2.0, 0.0, 0.0, 0.0, 1.0,
		1.0, 0.0, 2.0, 1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0,
		0.0, 0.0, 2.0, 1.0, 0.0, 2.0, 2.0, 0.0, 1.0, 0.0,
		2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 2.0, 2.0,
		1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 1.0,
		2.0, 1.0, 2.0, 2.0, 0.0, 0.0, 1.0, 2.0, 0.0, 2.0,
		2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 1.0, 2.0, 0.0,
		1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0,
		0.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 2.0,
		1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0,
		0.0, 1.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0,
		1.0, 2.0, 1.0, 1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0,
		0.0, 0.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 0.0, 2.0,
		0.0, 2.0, 1.0, 2.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0,
		2.0, 2.0, 1.0, 1.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0,
		1.0, 0.0, 1.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 1.0,
	];

			/*
from train sprial.py
	let data:Vec<f64> = {
		let data = [
			[ 0.662295,       0.3070266  ],
			[-0.4507611,     -0.3649855  ],
			[-0.29891637,    -0.75282735 ],
			[ 0.1297094,     -0.15223493 ],
			[-0.24501143,     0.04969303 ],
			[ 0.43561164,    -0.45512912 ],
			[-0.0,            0.0        ],
			[ 0.5703474,      0.5609847  ],
			[ 0.05997716,    -0.00165524 ],
			[-0.05060399,    -0.427012   ],
			[ 0.36321253,     0.62167245 ],
			[ 0.4752903,     -0.06707557 ],
			[-0.7781106,      0.05425706 ],
			[ 0.6052991,      0.5382499  ],
			[-0.10498876,     0.44785863 ],
			[-0.28640968,    -0.78966415 ],
			[ 0.02506495,     0.07597202 ],
			[-0.00198436,     0.5599965  ],
			[-0.83592904,     0.4075814  ],
			[-0.08773611,     0.4820813  ],
			[ 0.06264956,     0.6067743  ],
			[-0.7074377,     -0.13391    ],
			[ 0.42675576,    -0.05272124 ],
			[-0.56674325,    -0.16401847 ],
			[-0.16242598,    -0.40892273 ],
			[ 0.3031688,      0.06472    ],
			[-0.08871835,    -0.0151345  ],
			[-0.66352814,    -0.25264674 ],
			[-0.02386156,    -0.01818313 ],
			[-0.20607458,     0.95808834 ],
			[ 0.41908243,    -0.02774712 ],
			[ 0.30444184,    -0.5855896  ],
			[ 0.6181036,      0.442208   ],
			[ 0.05059152,    -0.30584392 ],
			[ 0.21005045,     0.15322797 ],
			[-0.81171393,     0.4113642  ],
			[-0.09433582,    -0.38871682 ],
			[ 0.27190292,     0.18699946 ],
			[-0.18813577,    -0.79812586 ],
			[ 0.2889293,     -0.40806845 ],
			[ 0.11015151,    -0.14236097 ],
			[ 0.9229654,      0.11417017 ],
			[-0.07488721,     0.51457936 ],
			[-0.08318522,     0.5031702  ],
			[ 0.03268636,    -0.2074406  ],
			[-0.21432792,     0.04963413 ],
			[-0.26522237,    -0.45886502 ],
			[-0.06727856,    -0.0193286  ],
			[-0.32124844,     0.07549458 ],
			[-0.8496624,     -0.02395374 ],
			[ 0.46213728,    -0.36678216 ],
			[-0.01839967,    -0.00783914 ],
			[ 0.17171463,     0.18169777 ],
			[ 0.25992823,     0.1497909  ],
			[-0.14541428,    -0.8374692  ],
			[ 0.90872765,     0.27697313 ],
			[-0.5675895,     -0.3560086  ],
			[ 0.36989382,    -0.00886326 ],
			[ 0.03873516,    -0.28740144 ],
			[-0.5865638,     -0.2800766  ],
			[-0.73952246,    -0.02658003 ],
			[ 0.13424662,    -0.21089771 ],
			[ 0.27801412,    -0.58754414 ],
			[-0.2739722,      0.14504905 ],
			[-0.17861071,     0.11044552 ],
			[ 0.10349102,    -0.74282545 ],
			[ 0.07829762,     0.13953309 ],
			[ 0.9365552,      0.08040111 ],
			[-0.29238784,     0.06715166 ],
			[ 0.34734264,     0.5494116  ],
			[-0.3786127,     -0.34169054 ],
			[ 0.044994,      -0.31682098 ],
			[-0.03366692,    -0.40861538 ],
			[ 0.2721572,      0.7416404  ],
			[ 0.10326176,    -0.09453575 ],
			[-0.2475858,      0.32680464 ],
			[-0.33446026,    -0.3981662  ],
			[ 0.13791645,    -0.76771027 ],
			[-0.23457344,    -0.4415601  ],
			[-0.12995958,    -0.00324155 ],
			[-0.07588156,    -0.02533749 ],
			[-0.33211905,    -0.42578977 ],
			[-0.7109096,      0.34453964 ],
			[ 0.02253613,     0.05560686 ],
			[ 0.10637081,    -0.10576035 ],
			[-0.6956357,     -0.22133903 ],
			[ 0.03228098,     0.11557655 ],
			[ 0.00094191,     0.04999113 ],
			[-0.2772819,      0.03891963 ],
			[ 0.01946609,    -0.35947332 ],
			[-0.5878122,     -0.22666465 ],
			[-0.6626818,     -0.19223109 ],
			[-0.10568184,    -0.03051801 ],
			[-0.04677129,    -0.01767616 ],
			[-0.30727252,     0.14554588 ],
			[-0.84679544,     0.35963517 ],
			[ 0.13746658,    -0.08187149 ],
			[ 0.1625132,     -0.68087405 ],
			[-0.7474886,      0.44515252 ],
			[ 0.42734203,    -0.10477977 ],
			[-0.43632174,    -0.8212937  ],
			[ 0.08143438,    -0.05803829 ],
			[-0.7848798,      0.154802   ],
			[ 0.08061991,     0.1609361  ],
			[ 0.49455667,    -0.3739167  ],
			[-0.11388239,    -0.86251426 ],
			[ 0.6075539,      0.47305202 ],
			[ 0.43614882,    -0.22332534 ],
			[-0.19780748,    -0.85748017 ],
			[-0.16978921,     0.00846301 ],
			[ 0.00652849,     0.0696949  ],
			[ 0.98899543,    -0.04458715 ],
			[ 0.09690688,    -0.24126554 ],
			[ 0.36761343,    -0.48678574 ],
			[-0.74260926,     0.47215626 ],
			[ 0.29977617,     0.11195645 ],
			[-0.23986334,     0.00809829 ],
			[ 0.3396491,      0.01544307 ],
			[-0.0401785,     -0.7989904  ],
			[-0.84386814,     0.282819   ],
			[-0.0,           -0.0        ],
			[-0.5489099,     -0.28826708 ],
			[ 0.38677302,    -0.34757248 ],
			[ 0.43358243,    -0.18140091 ],
			[-0.2656141,     -0.39981142 ],
			[-0.30824462,    -0.8028607  ],
			[-0.6003261,     -0.27424175 ],
			[ 0.0,           -0.0        ],
			[ 0.9177843,      0.3139619  ],
			[-0.7118476,      0.48256913 ],
			[ 0.3952812,     -0.45138982 ],
			[ 0.4547784,     -0.06911298 ],
			[ 0.06216975,    -0.03217021 ],
			[-0.14677872,     0.42538926 ],
			[ 0.07434364,    -0.02954696 ],
			[ 0.34992874,     0.6062589  ],
			[-0.15145358,     0.3375823  ],
			[ 0.2517982,      0.14386679 ],
			[ 0.15010694,    -0.17426391 ],
			[-0.00097699,     0.00995216 ],
			[-0.19678791,     0.03570037 ],
			[-0.25087747,     0.09980225 ],
			[ 0.7107062,      0.42871517 ],
			[-0.14588465,    -0.0348951  ],
			[ 0.15577695,    -0.25638554 ],
			[-0.23974584,     0.25499398 ],
			[ 0.46965688,    -0.19880246 ],
			[-0.23335491,     0.11465377 ],
			[-0.00570804,     0.5299693  ],
			[-0.08034188,    -0.45292956 ],
			[ 3.8451109e-02, -1.1023259e-02 ],
			[-7.6282126e-01,  1.0489876e-01 ],
			[-5.9743100e-01,  7.8941506e-01 ],
			[-1.3840458e-01,  3.7529212e-01 ],
			[ 3.7049392e-01,  5.8209473e-01 ],
			[ 3.7874952e-01,  3.0802788e-02 ],
			[-3.8604334e-04,  2.9997516e-02 ],
			[ 3.6799538e-01, -5.8367747e-01 ],
			[ 9.1857535e-01,  5.1179718e-02 ],
			[-2.8914639e-01,  2.6171428e-01 ],
			[ 9.7988080e-03, -1.9958380e-03 ],
			[ 8.9618134e-01,  8.2818918e-02 ],
			[ 7.3510563e-01,  4.0647230e-01 ],
			[-1.5293403e-01, -4.4442230e-01 ],
			[-5.7954365e-01, -2.7153113e-01 ],
			[ 4.9589045e-02, -6.3973782e-03 ],
			[ 3.8932335e-01, -2.2963692e-02 ],
			[ 9.2200153e-02, -5.9992772e-02 ],
			[ 3.5615346e-01,  5.2485455e-02 ],
			[ 5.0555551e-01,  5.9398115e-01 ],
			[ 9.3362927e-01, -2.9788658e-01 ],
			[-4.0200278e-01, -3.8986379e-01 ],
			[-1.4900257e-01,  5.2943200e-01 ],
			[-2.6835942e-01,  3.8585389e-01 ],
			[ 1.3923591e-01,  1.2928016e-01 ],
			[-5.8700913e-01, -7.9719526e-01 ],
			[-4.1255736e-01, -3.6372572e-01 ],
			[ 9.3739577e-02, -7.4919231e-02 ],
			[ 3.9775649e-01, -4.2305853e-02 ],
			[-7.2815371e-01, -5.1175404e-01 ],
			[-6.30262673e-01, 7.24133253e-01],
			[ 8.28423500e-02, 1.00185551e-01],
			[ 1.32737488e-01,-1.99951887e-01],
			[ 8.06974098e-02,-3.98488119e-02],
			[ 6.46809101e-01, 5.04021823e-01],
			[ 5.00808597e-01,-1.73466787e-01],
			[ 1.23994559e-01, 1.16298534e-01],
			[-5.13932295e-02,-3.09634637e-02],
			[ 8.14670503e-01, 2.75521219e-01],
			[-2.74970353e-01,-4.05575275e-01],
			[-5.75705886e-01, 7.80680954e-01],
			[ 9.50896204e-01,-1.31895423e-01],
			[-1.95264906e-01, 3.94298881e-01],
			[ 1.32352427e-01, 6.26165211e-01],
			[-4.84363623e-02,-3.46632242e-01],
			[ 8.45117390e-01, 2.06583202e-01],
			[ 1.99940726e-02, 4.86876263e-04],
			[-7.31852770e-01, 2.04918325e-01],
			[ 1.20922886e-01,-7.09772944e-01],
			[ 1.50631458e-01, 1.86842605e-01],
			[ 4.24040616e-01,-3.50270689e-01],
			[-6.50907755e-01, 6.21545732e-01],
			[ 1.25355586e-01, 5.86758852e-01],
			[-1.16656884e-03,-8.29999208e-01],
			[ 8.33525538e-01, 3.65150899e-01],
			[-2.67068833e-01, 2.70322472e-01],
			[-3.84353399e-02,-6.68896675e-01],
			[ 2.32019722e-01, 1.38082772e-01],
			[ 3.28234285e-01,-5.49419940e-01],
			[ 5.65227680e-02, 5.77239275e-01],
			[ 0.00491708,    -0.33996445    ],
			[-0.21861286,     0.35862017    ],
			[ 0.10985696,     0.08678392    ],
			[-0.20848857,     0.53050214    ],
			[ 0.39779076,    -0.09931022    ],
			[-0.0271639,     -0.02936192    ],
			[ 0.091654,      -0.1664318     ],
			[ 0.09607905,    -0.08757178    ],
			[-0.61125225,    -0.29794413    ],
			[-0.00640535,    -0.759973      ],
			[ 0.4110389,     -0.4092029     ],
			[ 0.02616625,    -0.01467403    ],
			[-0.19585939,    -0.88867265    ],
			[ 0.30133522,     0.541846      ],
			[-0.11285946,    -0.04077674    ],
			[ 0.06190507,     0.09092724    ],
			[ 0.09332082,     0.11743604    ],
			[ 0.01425096,     0.03737526    ],
			[ 0.13898164,     0.15742968    ],
			[-0.0468118,     -0.3871804     ],
			[ 0.5306231,     -0.20818052    ],
			[ 0.1374263,     -0.17179643    ],
			[-0.6833807,     -0.6883972     ],
			[ 0.05984409,     0.0801167     ],
			[-0.18852998,    -0.37530845    ],
			[-0.6994348,      0.0281238     ],
			[-0.18253154,     0.05274695    ],
			[-0.6793856,     -0.7062827     ],
			[-0.2701724,      0.1053892     ],
			[ 0.14116073,     0.18158647    ],
			[ 0.10375305,    -0.26006788    ],
			[-0.80911016,     0.1331945     ],
			[ 0.44608194,     0.6029187     ],
			[ 0.42443117,    -0.33385354    ],
			[-0.45640576,     0.83318293    ],
			[ 0.43733358,    -0.10601573    ],
			[-0.14406797,     0.65432745    ],
			[ 0.27577567,     0.56643426    ],
			[ 0.04370912,    -0.738708      ],
			[-0.21264821,    -0.39658636    ],
			[ 0.31820756,     0.6680898     ],
			[-0.17989218,     0.00622921    ],
			[-0.39722073,    -0.8519482     ],
			[-0.13981389,    -0.00721657    ],
			[-0.08959205,    -0.04442143    ],
			[-0.7121174,      0.42636698    ],
			[-0.02893586,     0.58929       ],
			[ 0.86612016,     0.20478243    ],
			[ 0.00246038,    -0.37999204    ],
			[-0.4918118,     -0.3608617     ],
			[-0.77509767,     0.23520966    ],
			[-0.22394885,     0.05241101    ],
			[-0.13912046,     0.48025566    ],
			[ 0.10108227,    -0.13668348    ],
			[-0.7847296,      0.29966557    ],
			[ 0.2805128,     -0.4846778     ],
			[-0.24811222,     0.26084542    ],
			[ 0.1811798,     -0.70715904    ],
			[ 0.18987328,     0.11112218    ],
			[-0.19354802,     0.4392484     ],
			[-0.17036279,    -0.75091714    ],
			[-0.15886047,    -0.01906173    ],
			[ 0.15359965,     0.12809038    ],
			[-0.9223507,      0.18129861    ],
			[ 0.49350116,     0.51044744    ],
			[-0.12524168,     0.41135693    ],
			[ 0.23121913,     0.1579168     ],
			[-0.52873564,    -0.7283122     ],
			[ 0.00512668,     0.01933176    ],
			[ 0.31354076,     0.58076864    ],
			[-0.41539234,    -0.43295404    ],
			[-0.74082303,    -0.59471107    ],
			[-0.01586872,     0.5397668     ],
			[ 0.4326635,     -0.5245973     ],
			[ 0.5026432,      0.4579845     ],
			[-0.28567183,     0.14419296    ],
			[-0.01549666,    -0.78984797    ],
			[-0.07018702,    -0.7065223     ],
			[-0.7831353,     -0.5552469     ],
			[ 0.02211946,    -0.2690924     ],
			[-0.74176145,     0.11086001    ],
			[-0.30477002,    -0.4816796     ],
			[ 0.02032876,     0.08767407    ],
			[-0.65559965,    -0.6454371     ],
			[ 0.6405069,      0.60344917    ],
			[ 0.6741878,      0.517659      ],
			[-0.1008854,     -0.35598052    ],
			[-0.01749143,    -0.3295361     ],
			[ 0.34195596,     0.07460645    ],
			[-0.00837021,    -0.00547171    ]
		];
		data.iter().fold(vec!(), |v, d| { let mut mut_v = v.clone();
										  mut_v.push(d[0]);
										  mut_v.push(d[1]);
										  mut_v })
	};
			let teacher:Vec<f64> = vec![ 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 0.0, 1.0, 2.0, 2.0,
								 1.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 1.0, 2.0, 1.0,
								 1.0, 2.0, 0.0, 2.0, 2.0, 0.0, 1.0, 2.0, 1.0, 2.0,
								 0.0, 0.0, 1.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0,
								 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0,
								 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 2.0, 2.0,
								 2.0, 2.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0,
								 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0,
								 1.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 1.0, 2.0,
								 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 0.0, 2.0, 0.0,
								 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0,
								 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 0.0, 2.0,
								 1.0, 2.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 1.0, 2.0,
								 0.0, 0.0, 2.0, 1.0, 2.0, 1.0, 1.0, 0.0, 2.0, 0.0,
								 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 1.0, 1.0, 2.0,
								 2.0, 2.0, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0,
								 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 2.0, 0.0, 1.0,
								 1.0, 2.0, 1.0, 1.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0,
								 2.0, 0.0, 2.0, 2.0, 1.0, 0.0, 0.0, 1.0, 1.0, 2.0,
								 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 0.0, 0.0,
								 0.0, 2.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0,
								 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 2.0, 2.0, 0.0,
								 0.0, 2.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 2.0,
								 0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 1.0, 0.0, 1.0, 0.0,
								 2.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.0, 2.0,
								 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0,
								 2.0, 1.0, 1.0, 2.0, 2.0, 0.0, 1.0, 0.0, 0.0, 1.0,
								 0.0, 1.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0,
								 2.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 2.0,
								 2.0, 2.0, 0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 0.0, 1.0 ];
	from spiral.py test
		let data:Vec<f64> = {
		let data = [
			[-1.39813885e-01, -7.21657369e-03],
			[ 3.70493919e-01,  5.82094729e-01],
			[ 1.37426302e-01, -1.71796426e-01],
			[ 3.03168803e-01,  6.47200048e-02],
			[-2.08488569e-01,  5.30502141e-01],
			[-7.07437694e-01, -1.33910000e-01],
			[ 4.94556665e-01, -3.73916686e-01],
			[ 2.32019722e-01,  1.38082772e-01],
			[-1.58860475e-01, -1.90617274e-02],
			[ 4.91708051e-03, -3.39964449e-01],
			[-1.29959583e-01, -3.24155297e-03],
			[-2.39745840e-01,  2.54993975e-01],
			[-3.32119048e-01, -4.25789773e-01],
			[-1.44067973e-01,  6.54327452e-01],
			[-8.77361074e-02,  4.82081294e-01],
			[-8.46795440e-01,  3.59635174e-01],
			[ 3.49928737e-01,  6.06258929e-01],
			[-2.65614092e-01, -3.99811417e-01],
			[ 3.87351550e-02, -2.87401438e-01],
			[-1.79892182e-01,  6.22920739e-03],
			[-2.65222371e-01, -4.58865017e-01],
			[-1.58687178e-02,  5.39766788e-01],
			[ 1.50631458e-01,  1.86842605e-01],
			[-7.78110623e-01,  5.42570576e-02],
			[-1.05681837e-01, -3.05180103e-02],
			[ 1.71714634e-01,  1.81697771e-01],
			[-5.06039858e-02, -4.27011997e-01],
			[-5.87812185e-01, -2.26664647e-01],
			[ 4.27342027e-01, -1.04779772e-01],
			[ 1.99940726e-02,  4.86876263e-04],
			[-6.55599654e-01, -6.45437121e-01],
			[ 9.33208242e-02,  1.17436044e-01],
			[-4.67712879e-02, -1.76761616e-02],
			[-5.87009132e-01, -7.97195256e-01],
			[ 1.89873278e-01,  1.11122184e-01],
			[-2.74970353e-01, -4.05575275e-01],
			[-6.62681818e-01, -1.92231089e-01],
			[ 8.96181345e-01,  8.28189179e-02],
			[-2.85671830e-01,  1.44192964e-01],
			[-6.95635676e-01, -2.21339032e-01],
			[-7.39522457e-01, -2.65800320e-02],
			[ 6.40506923e-01,  6.03449166e-01],
			[ 3.39649111e-01,  1.54430708e-02],
			[-6.40535168e-03, -7.59972990e-01],
			[-2.98916370e-01, -7.52827346e-01],
			[-1.93548024e-01,  4.39248413e-01],
			[ 4.37333584e-01, -1.06015727e-01],
			[ 9.36555207e-01,  8.04011077e-02],
			[-4.15392339e-01, -4.32954043e-01],
			[-1.51453584e-01,  3.37582290e-01],
			[ 1.38981640e-01,  1.57429680e-01],
			[-2.18612865e-01,  3.58620167e-01],
			[ 3.13540757e-01,  5.80768645e-01],
			[-5.97431004e-01,  7.89415061e-01],
			[ 2.10050449e-01,  1.53227970e-01],
			[-7.10909605e-01,  3.44539642e-01],
			[ 2.25361325e-02,  5.56068607e-02],
			[-3.08244616e-01, -8.02860677e-01],
			[-8.31852183e-02,  5.03170192e-01],
			[-7.47488618e-01,  4.45152521e-01],
			[ 1.55776948e-01, -2.56385535e-01],
			[ 3.47342640e-01,  5.49411595e-01],
			[-6.63528144e-01, -2.52646744e-01],
			[-5.13932295e-02, -3.09634637e-02],
			[-6.79385602e-01, -7.06282675e-01],
			[-1.54966572e-02, -7.89847970e-01],
			[ 4.46081936e-01,  6.02918684e-01],
			[ 6.26495630e-02,  6.06774271e-01],
			[-1.70362785e-01, -7.50917137e-01],
			[ 1.42509649e-02,  3.73752601e-02],
			[-5.70803555e-03,  5.29969275e-01],
			[-3.21248442e-01,  7.54945800e-02],
			[ 1.32737488e-01, -1.99951887e-01],
			[ 5.70347428e-01,  5.60984671e-01],
			[ 9.69068781e-02, -2.41265535e-01],
			[ 9.16540027e-02, -1.66431800e-01],
			[-7.75097668e-01,  2.35209659e-01],
			[ 6.62294984e-01,  3.07026595e-01],
			[ 4.62137282e-01, -3.66782159e-01],
			[ 7.43436441e-02, -2.95469631e-02],
			[ 4.35611635e-01, -4.55129117e-01],
			[-1.95264906e-01,  3.94298881e-01],
			[ 2.71902919e-01,  1.86999455e-01],
			[-6.83380723e-01, -6.88397229e-01],
			[-2.68359423e-01,  3.85853887e-01],
			[-1.97807476e-01, -8.57480168e-01],
			[-5.67589521e-01, -3.56008589e-01],
			[-6.11252248e-01, -2.97944129e-01],
			[-2.45011434e-01,  4.96930256e-02],
			[-4.02002782e-01, -3.89863789e-01],
			[-8.87183547e-02, -1.51345013e-02],
			[ 8.28423500e-02,  1.00185551e-01],
			[-3.97220731e-01, -8.51948202e-01],
			[ 1.41160727e-01,  1.81586474e-01],
			[-3.86043335e-04,  2.99975164e-02],
			[-1.88135773e-01, -7.98125863e-01],
			[ 9.41914099e-04,  4.99911271e-02],
			[-2.34573439e-01, -4.41560090e-01],
			[ 4.37091216e-02, -7.38708019e-01],
			[ 1.03491016e-01, -7.42825449e-01],
			[-1.13882385e-01, -8.62514257e-01],
			[ 4.24040616e-01, -3.50270689e-01],
			[-6.30262673e-01,  7.24133253e-01],
			[ 2.75775671e-01,  5.66434264e-01],
			[ 2.03287583e-02,  8.76740664e-02],
			[-4.91811812e-01, -3.60861689e-01],
			[ 4.75290298e-01, -6.70755729e-02],
			[ 8.45117390e-01,  2.06583202e-01],
			[ 7.82976225e-02,  1.39533088e-01],
			[ 9.33629274e-01, -2.97886580e-01],
			[ 3.28234285e-01, -5.49419940e-01],
			[ 3.86773020e-01, -3.47572476e-01],
			[ 1.34246618e-01, -2.10897714e-01],
			[ 1.62513196e-01, -6.80874050e-01],
			[-8.09110165e-01,  1.33194506e-01],
			[ 4.24431175e-01, -3.33853543e-01],
			[-2.38615572e-02, -1.81831270e-02],
			[ 1.20922886e-01, -7.09772944e-01],
			[-2.86409676e-01, -7.89664149e-01],
			[-7.28153706e-01, -5.11754036e-01],
			[-7.62821257e-01,  1.04898758e-01],
			[-7.31852770e-01,  2.04918325e-01],
			[-1.82531536e-01,  5.27469479e-02],
			[ 8.66120160e-01,  2.04782426e-01],
			[ 1.81179807e-01, -7.07159042e-01],
			[-1.12859458e-01, -4.07767445e-02],
			[ 3.67613435e-01, -4.86785740e-01],
			[ 1.23994559e-01,  1.16298534e-01],
			[ 8.06974098e-02, -3.98488119e-02],
			[ 6.46809101e-01,  5.04021823e-01],
			[ 5.12668258e-03,  1.93317644e-02],
			[-1.04988761e-01,  4.47858632e-01],
			[-8.95920545e-02, -4.44214307e-02],
			[ 7.35105634e-01,  4.06472296e-01],
			[ 2.99776167e-01,  1.11956447e-01],
			[ 9.22001526e-02, -5.99927716e-02],
			[-5.75705886e-01,  7.80680954e-01],
			[-7.48872086e-02,  5.14579356e-01],
			[ 1.39235914e-01,  1.29280165e-01],
			[ 1.32352427e-01,  6.26165211e-01],
			[ 9.17784274e-01,  3.13961893e-01],
			[ 9.08727646e-01,  2.76973128e-01],
			[-3.34460258e-01, -3.98166209e-01],
			[-1.46778718e-01,  4.25389260e-01],
			[ 9.50896204e-01, -1.31895423e-01],
			[ 8.14670503e-01,  2.75521219e-01],
			[-1.69789210e-01,  8.46300833e-03],
			[ 8.33525538e-01,  3.65150899e-01],
			[-5.79543650e-01, -2.71531135e-01],
			[ 2.80512810e-01, -4.84677792e-01],
			[ 2.21194550e-02, -2.69092411e-01],
			[-1.83996726e-02, -7.83913676e-03],
			[ 8.14343765e-02, -5.80382869e-02],
			[-8.11713934e-01,  4.11364198e-01],
			[ 4.49940041e-02, -3.16820979e-01],
			[ 3.78749520e-01,  3.08027882e-02],
			[ 4.26755756e-01, -5.27212434e-02],
			[-7.12117374e-01,  4.26366985e-01],
			[ 5.00808597e-01, -1.73466787e-01],
			[-2.33354911e-01,  1.14653774e-01],
			[ 8.06199089e-02,  1.60936102e-01],
			[ 3.04441839e-01, -5.85589588e-01],
			[-7.40823030e-01, -5.94711065e-01],
			[ 9.88995433e-01, -4.45871502e-02],
			[-6.50907755e-01,  6.21545732e-01],
			[ 4.69656885e-01, -1.98802456e-01],
			[-7.58815631e-02, -2.53374856e-02],
			[-8.03418756e-02, -4.52929556e-01],
			[ 9.79880802e-03, -1.99583801e-03],
			[ 6.18103623e-01,  4.42207992e-01],
			[ 4.19082433e-01, -2.77471188e-02],
			[-2.92387843e-01,  6.71516582e-02],
			[ 4.54778403e-01, -6.91129789e-02],
			[ 1.94660928e-02, -3.59473318e-01],
			[ 3.69893819e-01, -8.86326004e-03],
			[-0.00000000e+00, -0.00000000e+00],
			[-1.16656884e-03, -8.29999208e-01],
			[ 7.10706174e-01,  4.28715169e-01],
			[ 5.98440878e-02,  8.01166967e-02],
			[ 1.03753053e-01, -2.60067880e-01],
			[-7.84729600e-01,  2.99665570e-01],
			[-3.04770023e-01, -4.81679589e-01],
			[ 3.26863602e-02, -2.07440600e-01],
			[ 2.31219128e-01,  1.57916799e-01],
			[-8.43868136e-01,  2.82819003e-01],
			[ 1.53599650e-01,  1.28090382e-01],
			[ 5.05555511e-01,  5.93981147e-01],
			[-2.23948851e-01,  5.24110086e-02],
			[ 3.97756487e-01, -4.23058532e-02],
			[-3.84353399e-02, -6.68896675e-01],
			[-1.25241682e-01,  4.11356926e-01],
			[ 9.37395766e-02, -7.49192312e-02],
			[ 3.18207562e-01,  6.68089807e-01],
			[ 5.65227680e-02,  5.77239275e-01],
			[-2.89146394e-01,  2.61714280e-01],
			[-4.68118005e-02, -3.87180388e-01],
			[-5.86563826e-01, -2.80076593e-01],
			[-7.41761446e-01,  1.10860012e-01],
			[-1.52934030e-01, -4.44422305e-01],
			[-3.07272524e-01,  1.45545885e-01],
			[ 1.25355586e-01,  5.86758852e-01],
			[ 5.05915172e-02, -3.05843920e-01],
			[-2.70172387e-01,  1.05389200e-01],
			[-7.84879804e-01,  1.54801995e-01],
			[-2.71639023e-02, -2.93619204e-02],
			[ 3.67995381e-01, -5.83677471e-01],
			[ 1.37916446e-01, -7.67710268e-01],
			[-8.49662423e-01, -2.39537414e-02],
			[-9.22350705e-01,  1.81298614e-01],
			[ 2.46037939e-03, -3.79992038e-01],
			[-4.01784964e-02, -7.98990428e-01],
			[-2.14327917e-01,  4.96341288e-02],
			[ 1.06370807e-01, -1.05760351e-01],
			[ 5.30623078e-01, -2.08180517e-01],
			[-2.50877470e-01,  9.98022482e-02],
			[-2.89358608e-02,  5.89290023e-01],
			[ 3.01335216e-01,  5.41845977e-01],
			[-2.67068833e-01,  2.70322472e-01],
			[-8.37020576e-03, -5.47171477e-03],
			[ 0.00000000e+00, -0.00000000e+00],
			[-1.00885399e-01, -3.55980515e-01],
			[-2.47585803e-01,  3.26804638e-01],
			[ 1.10151507e-01, -1.42360970e-01],
			[ 3.89323354e-01, -2.29636915e-02],
			[-9.76986485e-04,  9.95216053e-03],
			[ 1.37466580e-01, -8.18714872e-02],
			[ 4.32663500e-01, -5.24597287e-01],
			[-4.36321735e-01, -8.21293712e-01],
			[ 2.78014123e-01, -5.87544143e-01],
			[ 1.29709393e-01, -1.52234927e-01],
			[ 6.52848883e-03,  6.96948990e-02],
			[-6.72785640e-02, -1.93286035e-02],
			[-1.95859388e-01, -8.88672650e-01],
			[ 9.60790515e-02, -8.75717774e-02],
			[ 1.09856956e-01,  8.67839158e-02],
			[-6.00326121e-01, -2.74241745e-01],
			[ 6.74187779e-01,  5.17659009e-01],
			[ 6.05299115e-01,  5.38249910e-01],
			[-1.49002567e-01,  5.29431999e-01],
			[-3.78612697e-01, -3.41690540e-01],
			[-1.38404578e-01,  3.75292122e-01],
			[-1.39120460e-01,  4.80255663e-01],
			[-1.45884648e-01, -3.48950997e-02],
			[-7.83135295e-01, -5.55246890e-01],
			[-5.28735638e-01, -7.28312194e-01],
			[-2.77281910e-01,  3.89196314e-02],
			[ 3.63212526e-01,  6.21672451e-01],
			[ 4.33582425e-01, -1.81400910e-01],
			[-2.39863336e-01,  8.09829123e-03],
			[-6.99434817e-01,  2.81237978e-02],
			[-9.43358168e-02, -3.88716817e-01],
			[-2.73972213e-01,  1.45049050e-01],
			[ 1.01082273e-01, -1.36683479e-01],
			[ 2.59928226e-01,  1.49790898e-01],
			[ 2.61662528e-02, -1.46740330e-02],
			[-1.78610712e-01,  1.10445522e-01],
			[ 1.03261761e-01, -9.45357531e-02],
			[-4.50761110e-01, -3.64985496e-01],
			[-7.42609262e-01,  4.72156256e-01],
			[-1.74914338e-02, -3.29536110e-01],
			[ 2.88929313e-01, -4.08068448e-01],
			[-7.11847603e-01,  4.82569128e-01],
			[-0.00000000e+00,  0.00000000e+00],
			[ 5.02643228e-01,  4.57984507e-01],
			[ 3.95281196e-01, -4.51389819e-01],
			[-2.12648213e-01, -3.96586359e-01],
			[ 3.41955960e-01,  7.46064484e-02],
			[-7.01870173e-02, -7.06522286e-01],
			[ 6.19050711e-02,  9.09272358e-02],
			[ 2.72157192e-01,  7.41640389e-01],
			[-4.84363623e-02, -3.46632242e-01],
			[ 4.93501157e-01,  5.10447443e-01],
			[ 9.22965407e-01,  1.14170171e-01],
			[-1.62425980e-01, -4.08922732e-01],
			[-1.98435900e-03,  5.59996486e-01],
			[-3.36669162e-02, -4.08615381e-01],
			[-1.96787909e-01,  3.57003734e-02],
			[ 3.97790760e-01, -9.93102193e-02],
			[ 9.18575346e-01,  5.11797182e-02],
			[ 4.95890453e-02, -6.39737817e-03],
			[-5.48909903e-01, -2.88267076e-01],
			[-4.12557364e-01, -3.63725722e-01],
			[ 3.56153458e-01,  5.24854548e-02],
			[ 5.99771626e-02, -1.65524380e-03],
			[-1.45414278e-01, -8.37469220e-01],
			[ 3.84511091e-02, -1.10232588e-02],
			[ 3.22809778e-02,  1.15576550e-01],
			[ 6.07553899e-01,  4.73052025e-01],
			[ 2.51798213e-01,  1.43866792e-01],
			[-2.48112217e-01,  2.60845423e-01],
			[ 1.50106937e-01, -1.74263909e-01],
			[ 6.21697456e-02, -3.21702138e-02],
			[-4.56405759e-01,  8.33182931e-01],
			[ 4.11038905e-01, -4.09202904e-01],
			[-2.06074581e-01,  9.58088338e-01],
			[-5.66743255e-01, -1.64018467e-01],
			[-8.35929036e-01,  4.07581389e-01],
			[ 4.36148822e-01, -2.23325342e-01],
			[-1.88529983e-01, -3.75308454e-01],
			[ 2.50649527e-02,  7.59720206e-02]];
		data.iter().fold(vec!(), |v, d| { let mut mut_v = v.clone();
										  mut_v.push(d[0]);
										  mut_v.push(d[1]);
										  mut_v })
	};
	let teacher:Vec<f64> = vec![1.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 0.0, 1.0, 2.0,
								1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0,
								2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0,
								0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 2.0, 1.0, 1.0, 2.0,
								2.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0,
								0.0, 1.0, 1.0, 2.0, 0.0, 2.0, 0.0, 0.0, 1.0, 2.0,
								2.0, 1.0, 2.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0,
								1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 0.0, 2.0,
								0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0, 2.0, 1.0, 2.0,
								1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0,
								0.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0,
								0.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 0.0, 0.0,
								2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 2.0, 1.0,
								0.0, 1.0, 1.0, 1.0, 0.0, 2.0, 2.0, 1.0, 0.0, 1.0,
								1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 0.0,
								2.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0,
								0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 2.0, 1.0,
								0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0,
								2.0, 2.0, 2.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.0, 0.0,
								1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0,
								1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 0.0, 2.0, 2.0, 2.0,
								0.0, 1.0, 2.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0,
								2.0, 1.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0,
								0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 1.0, 1.0, 2.0,
								1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 2.0,
								2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0,
								0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0,
								2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0,
								2.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0,
								2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 0.0, 2.0, 0.0];
		 */
	(Tensor::<f64>::from_vector(vec![300,2],data),Tensor::<f64>::from_vector(vec![300,1],teacher))
}

fn print_data_graph(filename:&str, xs:&Tensor::<f64>, ts:&Tensor::<f64>) -> Result<(),Box<dyn std::error::Error>> {

	let render_backend = BitMapBackend::new(filename, (640, 480)).into_drawing_area();
	render_backend.fill(&WHITE)?;
	let mut chart_builder = ChartBuilder::on(&render_backend)
		.caption("spiral", ("sans-serif", 40).into_font())
		.margin(5)
		.x_label_area_size(30)
		.y_label_area_size(30)
		.build_cartesian_2d(-1.56f32..1.56f32, -1.2f32..1.2f32)?;
	chart_builder.configure_mesh().disable_x_mesh().disable_y_mesh().draw()?;
	let class_iter =
		xs.iter().zip(ts.iter()).filter(|(_xst,t)| t[vec![0,0]] == 0.0 as f64);
	chart_builder.draw_series(class_iter.map(|(xst,_t)| {
		let x = xst[vec![0,0]] as f32;
		let y = xst[vec![0,1]] as f32;
		EmptyElement::at((x,y)) + Circle::new((0,0), 2, GREEN.filled())
	}))?;
	let class_iter =
			xs.iter().zip(ts.iter()).filter(|(_xst,t)| t[vec![0,0]] == 1.0 as f64);
	chart_builder.draw_series(class_iter.map(|(xst,_t)| {
		let x = xst[vec![0,0]] as f32;
		let y = xst[vec![0,1]] as f32;
		EmptyElement::at((x,y)) + TriangleMarker::new((0,0), 2, RED.filled())
	}))?;
	let class_iter =
		xs.iter().zip(ts.iter()).filter(|(_xst,t)| t[vec![0,0]] == 2.0 as f64);
	chart_builder.draw_series(class_iter.map(|(xst,_t)| {
		let x = xst[vec![0,0]] as f32;
		let y = xst[vec![0,1]] as f32;
		EmptyElement::at((x,y)) + Cross::new((0,0), 2, CYAN.filled())
	}))?;
	render_backend.present()?;

	Ok(())
}

#[test]
fn known_answer_spiral_classification_test() -> Result<(),Box<dyn std::error::Error>> {

	let max_epoch = 300;
	let batch_size = 30;
	let num_of_class = 3;
	let num_of_data = 100;
	let num_of_alldata = num_of_class * num_of_data;

	let (xs_orgin,ts_orgin) = fixed_sprial_data_set();
	let xs = Tensor::<f64>::from(xs_orgin);
	let ts = Tensor::<f64>::from(ts_orgin);
	print_data_graph("spiral_graph.png", &xs, &ts)?;

	let mut nn = NeuralNetwork::<f64>::new();
	let mut mlp_model = nn.create_mlp_model("MLP1", &[10,num_of_class], MLPActivator::Sigmoid);

	let w1 = [-0.4556128,  -0.50568277, -0.38025308,  0.30358982, -0.780519,
			  -0.69417423,  1.379786,    0.08218841,  0.5945719,   1.923685,
			  -0.7951241,   0.08588953, -0.0947597,  -0.36235344, -0.7156339,
			   0.10670566,  0.28559017,  0.6298104,  -1.2524264,   0.74073964];
	let b1 = [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ];
	let w2 =  [ 0.29730973,  0.5088898,  -0.21237679,
			   -0.08617237, -0.65514934, -0.20139706,
				0.32803735, -0.06675702, -0.7313797,
			   -0.37872538, -0.41369444,  0.33583945,
				0.03708619,  0.06745199, -0.264514,
			   -0.3879108,  -0.28312945, -0.39559156,
			   -0.2872457,   0.23332103, -0.37998372,
			   -0.17970745,  0.29457662, -0.13430262,
			   -0.3551994,  -0.32869664,  0.3206281,
			   -0.6150363,  -0.3387977,   0.32395628 ];
	let b2 = [ 0.0, 0.0, 0.0 ];

	let w1t = Tensor::<f64>::from_array(&[ 2,10], &w1);
	let b1t = Tensor::<f64>::from_array(&[ 1,10], &b1);
	let w2t = Tensor::<f64>::from_array(&[10, 3], &w2);
	let b2t = Tensor::<f64>::from_array(&[ 1, 3], &b2);

	let x = nn.create_constant("input_x", Tensor::<f64>::zero(&[batch_size,2]));
	let t = nn.create_constant("teacher", Tensor::<f64>::zero(&[batch_size,1]));
	let classfied_result = nn.model_set_weight_and_inputs(&mut mlp_model,
														  &[(w1t,Some(b1t)),(w2t,Some(b2t))],
														  vec![Rc::clone(&x)]);
	let mut optimizer = NNOptimizer::<f64>::new(Optimizer::SGD(SGD::new(1.0)),
												&mlp_model);
	let loss = nn.softmax_cross_entropy_error(Rc::clone(&classfied_result[0]), Rc::clone(&t));
	//let index:[usize;300] = core::array::from_fn(|i| i);

	let mlp_params = mlp_model.get_params();
	println!("w1 {}", mlp_params[0].borrow());
	println!("b1 {}", mlp_params[1].borrow());
	println!("w2 {}", mlp_params[2].borrow());
	println!("b2 {}", mlp_params[3].borrow());

	nn.backward_propagating(0)?;

	nn.make_dot_graph(0,"order0.dot")?;
	nn.make_dot_graph(1,"order1.dot")?;

	let count_of_training =  num_of_alldata/batch_size;

	for epoch in 0..max_epoch {
		let mut sum_loss:f64 = 0.0;
		let perm_table = {
			let mut v = (0..(num_of_data*num_of_class)).collect::<Vec<usize>>();
			v.shuffle(&mut nn.get_rng());
			v
		};

		for i in 0..count_of_training {
			let batch_index = &perm_table[i*batch_size..(i+1)*batch_size];
			let batch_x = xs.selector(batch_index);
			let batch_t = ts.selector(batch_index);

			//println!("batch_x {}",batch_x);
			//println!("batch_t {}",batch_t);
			x.borrow_mut().assign(batch_x.clone());
			t.borrow_mut().assign(batch_t.clone());

			//let w1_bind = mlp_params[0].borrow();
			//let b1_bind = mlp_params[1].borrow();
			//let w2_bind = mlp_params[2].borrow();
			//let b2_bind = mlp_params[3].borrow();
			//println!("forward prop 0");
			nn.forward_propagating(0)?;
			//println!("classfied_result: {:?}", Rc::as_ptr(&classfied_result[0]));
			//println!("forward prop 1");
			nn.forward_propagating(1)?;
			//println!("classfied_result: {:?}", Rc::as_ptr(&classfied_result[0]));

			let nn_model_w1 = |w1:&Tensor::<f64>| -> Tensor::<f64> {
				let b1_bind = mlp_params[1].borrow();
				let w2_bind = mlp_params[2].borrow();
				let b2_bind = mlp_params[3].borrow();

				let b1 = b1_bind.ref_signal();
				let w2 = w2_bind.ref_signal();
				let b2 = b2_bind.ref_signal();

				let l1 = {
					let result_shape = vec![batch_x.shape()[0], w1.shape()[1]];
					let b = b1.broadcast(&result_shape);
					let l1 = Tensor::<f64>::matrix_product(&batch_x, w1) + b;
					l1.sigmoid()
				};

				let result_shape = vec![l1.shape()[0], w2.shape()[1]];
				let b = b2.broadcast(&result_shape);
				let l2 = Tensor::<f64>::matrix_product(&l1, w2) + b;

				let maxs = l2.max_in_axis(1);
				let s = (&l2 - maxs.broadcast(l2.shape())).exp().sum_axis(1).ln();
				let log_z = (&maxs + s).broadcast(l2.shape());
				let log_p = &l2 - log_z;
				let log_p_v = (0..(l2.shape()[0])).zip(batch_t.buffer().iter()).map(|(l,t)| {
					log_p[vec![l.to_usize().unwrap(), t.to_usize().unwrap()]]
				}).collect::<Vec<f64>>();
				let log_p_sum:f64 = log_p_v.iter().fold(num::zero(), |s:f64,&e| { s + e });
				Tensor::<f64>::from_array(&[1,1],&[(-log_p_sum)/(l2.shape()[0] as f64)])
			};

			let nn_model_b1 = |b1:&Tensor::<f64>| -> Tensor::<f64> {
				let w1_bind = mlp_params[0].borrow();
				let w2_bind = mlp_params[2].borrow();
				let b2_bind = mlp_params[3].borrow();

				let w1 = w1_bind.ref_signal();
				let w2 = w2_bind.ref_signal();
				let b2 = b2_bind.ref_signal();

				let l1 = {
					let result_shape = vec![batch_x.shape()[0], w1.shape()[1]];
					let b = b1.broadcast(&result_shape);
					let l1 = Tensor::<f64>::matrix_product(&batch_x, w1) + b;
					l1.sigmoid()
				};

				let result_shape = vec![l1.shape()[0], w2.shape()[1]];
				let b = b2.broadcast(&result_shape);
				let l2 = Tensor::<f64>::matrix_product(&l1, w2) + b;

				let maxs = l2.max_in_axis(1);
				let s = (&l2 - maxs.broadcast(l2.shape())).exp().sum_axis(1).ln();
				let log_z = (&maxs + s).broadcast(l2.shape());
				let log_p = &l2 - log_z;
				let log_p_v = (0..(l2.shape()[0])).zip(batch_t.buffer().iter()).map(|(l,t)| {
					log_p[vec![l.to_usize().unwrap(), t.to_usize().unwrap()]]
				}).collect::<Vec<f64>>();
				let log_p_sum:f64 = log_p_v.iter().fold(num::zero(), |s:f64,&e| { s + e });
				Tensor::<f64>::from_array(&[1,1],&[(-log_p_sum)/(l2.shape()[0] as f64)])
			};

			let nn_model_w2 = |w2:&Tensor::<f64>| -> Tensor::<f64> {
				let w1_bind = mlp_params[0].borrow();
				let b1_bind = mlp_params[1].borrow();
				let b2_bind = mlp_params[3].borrow();

				let w1 = w1_bind.ref_signal();
				let b1 = b1_bind.ref_signal();
				let b2 = b2_bind.ref_signal();

				let l1 = {
					let result_shape = vec![batch_x.shape()[0], w1.shape()[1]];
					let b = b1.broadcast(&result_shape);
					let l1 = Tensor::<f64>::matrix_product(&batch_x, w1) + b;
					l1.sigmoid()
				};

				let result_shape = vec![l1.shape()[0], w2.shape()[1]];
				let b = b2.broadcast(&result_shape);
				let l2 = Tensor::<f64>::matrix_product(&l1, w2) + b;

				let maxs = l2.max_in_axis(1);
				let s = (&l2 - maxs.broadcast(l2.shape())).exp().sum_axis(1).ln();
				let log_z = (&maxs + s).broadcast(l2.shape());
				let log_p = &l2 - log_z;
				let log_p_v = (0..(l2.shape()[0])).zip(batch_t.buffer().iter()).map(|(l,t)| {
					log_p[vec![l.to_usize().unwrap(), t.to_usize().unwrap()]]
				}).collect::<Vec<f64>>();
				let log_p_sum:f64 = log_p_v.iter().fold(num::zero(), |s:f64,&e| { s + e });
				Tensor::<f64>::from_array(&[1,1],&[(-log_p_sum)/(l2.shape()[0] as f64)])
			};

			let nn_model_b2 = |b2:&Tensor::<f64>| -> Tensor::<f64> {
				let w1_bind = mlp_params[0].borrow();
				let b1_bind = mlp_params[1].borrow();
				let w2_bind = mlp_params[2].borrow();

				let w1 = w1_bind.ref_signal();
				let b1 = b1_bind.ref_signal();
				let w2 = w2_bind.ref_signal();

				let l1 = {
					let result_shape = vec![batch_x.shape()[0], w1.shape()[1]];
					let b = b1.broadcast(&result_shape);
					let l1 = Tensor::<f64>::matrix_product(&batch_x, w1) + b;
					l1.sigmoid()
				};

				let result_shape = vec![l1.shape()[0], w2.shape()[1]];
				let b = b2.broadcast(&result_shape);
				let l2 = Tensor::<f64>::matrix_product(&l1, w2) + b;

				let maxs = l2.max_in_axis(1);
				let s = (&l2 - maxs.broadcast(l2.shape())).exp().sum_axis(1).ln();
				let log_z = (&maxs + s).broadcast(l2.shape());
				let log_p = &l2 - log_z;
				let log_p_v = (0..(l2.shape()[0])).zip(batch_t.buffer().iter()).map(|(l,t)| {
					log_p[vec![l.to_usize().unwrap(), t.to_usize().unwrap()]]
				}).collect::<Vec<f64>>();
				let log_p_sum:f64 = log_p_v.iter().fold(num::zero(), |s:f64,&e| { s + e });
				Tensor::<f64>::from_array(&[1,1],&[(-log_p_sum)/(l2.shape()[0] as f64)])
			};

			let num_gw1 = numerical_grad(&nn_model_w1,
										 mlp_params[0].borrow().ref_signal(),
										 1.0e-6);
			let num_gb1 = numerical_grad(&nn_model_b1,
										 mlp_params[1].borrow().ref_signal(),
										 1.0e-6);
			let num_gw2 = numerical_grad(&nn_model_w2,
										 mlp_params[2].borrow().ref_signal(),
										 1.0e-6);
			let num_gb2 = numerical_grad(&nn_model_b2,
										 mlp_params[3].borrow().ref_signal(),
										 1.0e-6);

			if let Some(gw1) = mlp_params[0].borrow().ref_grad(){
				let result =
					gw1.borrow().ref_signal().buffer().iter().zip(num_gw1.buffer().iter()).fold(true,|r, (dx0,dx1)| r && ((dx0-dx1).abs() <= 1.0e-5));
				if !result {
					return Err(Box::new(MyError::StringMsg("gw1 is incorrect".to_string())));
				}
			}
			else {
				return Err(Box::new(MyError::StringMsg("gw1 is none".to_string())));
			};

			if let Some(gb1) = mlp_params[1].borrow().ref_grad(){
				let result =
					gb1.borrow().ref_signal().buffer().iter().zip(num_gb1.buffer().iter()).fold(true,|r, (dx0,dx1)| r && ((dx0-dx1).abs() <= 1.0e-5));
				if !result {
					return Err(Box::new(MyError::StringMsg("gb1 is incorrect".to_string())));
				}
			}
			else {
				return Err(Box::new(MyError::StringMsg("gb1 is none".to_string())));
			};

			if let Some(gw2) = mlp_params[2].borrow().ref_grad(){
				let result =
					gw2.borrow().ref_signal().buffer().iter().zip(num_gw2.buffer().iter()).fold(true,|r, (dx0,dx1)| r && ((dx0-dx1).abs() <= 1.0e-5));
				if !result {
					return Err(Box::new(MyError::StringMsg("gw2 is incorrect".to_string())));
				}
			}
			else {
				return Err(Box::new(MyError::StringMsg("gw2 is none".to_string())));
			};

			if let Some(gb2) = mlp_params[3].borrow().ref_grad(){
				let result =
					gb2.borrow().ref_signal().buffer().iter().zip(num_gb2.buffer().iter()).fold(true,|r, (dx0,dx1)| r && ((dx0-dx1).abs() <= 1.0e-5));
				if !result {
					return Err(Box::new(MyError::StringMsg("gb2 is incorrect".to_string())));
				}
			}
			else {
				return Err(Box::new(MyError::StringMsg("gb2 is none".to_string())));
			};
/*
			println!("y {}", classfied_result[0].borrow());
			if let Some(gy) = classfied_result[0].borrow().ref_grad() {
				println!("gy {}", gy.borrow());
			}
*/
			println!("loss {}",loss.borrow().ref_signal()[vec![0,0]]);
			sum_loss += loss.borrow().ref_signal()[vec![0,0]] * (batch_size as f64);
			optimizer.update()?;
		}
		println!("epoch {} sum loss:{}", epoch, sum_loss/num_of_alldata as f64);
	}

	Ok(())
}

